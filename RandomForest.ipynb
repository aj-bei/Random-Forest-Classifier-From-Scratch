{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3c3af331-4421-4088-bae6-62406bf86c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b0b847e9-dc7d-4371-95ce-bea13e072951",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier():\n",
    "\n",
    "    def __init__(self, num_trees=100, min_samples_split=2,max_depth=4, max_features='sqrt'):\n",
    "\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.base_learners = [] #holds list of decision tree objects\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    def bootstrap(self, dataset):\n",
    "\n",
    "        \"\"\"\n",
    "        Makes a boostrapped dataset to train decision tree on. Bootstrapping data in an ensemble model introduces \n",
    "        randomness to avoid overfitting\n",
    "        \"\"\"\n",
    "\n",
    "        bootstrapped_dataset = []\n",
    "\n",
    "        for _ in range(len(dataset)):\n",
    "\n",
    "            # grabs random row index from dataset\n",
    "            random_row_idx = np.random.randint(0, len(dataset))\n",
    "\n",
    "            bootstrapped_dataset.append(dataset[random_row_idx])\n",
    "\n",
    "        return np.array(bootstrapped_dataset)\n",
    "\n",
    "    def fit(self, dataset_features, dataset_targets):\n",
    "\n",
    "        # finding the number of features to use to train decision trees\n",
    "        if self.max_features == \"sqrt\":\n",
    "            max_features = int(math.pow(len(dataset_features[0]), 0.5))\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = int(math.log2(len(dataset_features[0])))\n",
    "        else:\n",
    "            max_features = self.max_features\n",
    "\n",
    "        # check if max_features is greater than the number of features in dataset\n",
    "        if max_features > dataset_features.shape[1]:\n",
    "            raise ValueError(f\"max_features ({max_features}) cannot be greater than the number of features in dataset({dataset_features.shape[1]})\")\n",
    "            \n",
    "        # training base learners\n",
    "        for _ in range(self.num_trees):\n",
    "\n",
    "            # get a bootstrapped dataset\n",
    "            bootstrapped_dataset = self.bootstrap(np.column_stack((dataset_features, dataset_targets)))\n",
    "            training_dataset_x = bootstrapped_dataset[:, :-1]\n",
    "            training_dataset_y = bootstrapped_dataset[:, -1]\n",
    "\n",
    "            # get random max_features amount of features to train the model\n",
    "            random_features = np.array(random.sample(range(len(dataset_features[0])), max_features))\n",
    "            training_dataset_x = training_dataset_x[:, random_features]\n",
    "\n",
    "            # instantiate and fit new decision tree model\n",
    "            decision_tree = DecisionTreeClassifier(self.min_samples_split, self.max_depth)\n",
    "            decision_tree.fit(training_dataset_x, training_dataset_y)\n",
    "\n",
    "            # add the trained decision tree to the list of base_learners along with the features used to train it\n",
    "            self.base_learners.append((decision_tree, random_features))\n",
    "\n",
    "    def make_prediction(self, X):\n",
    "\n",
    "        '''Get prediction for 1d array. Returns the most common predicted class from the decision tree base learners'''\n",
    "\n",
    "        predicted_classes = [] # stores the predicted class from each decision tree\n",
    "\n",
    "        for decision_tree, random_features in self.base_learners:\n",
    "\n",
    "            # filter X to only include features that the decision tree was trained on \n",
    "            X_filtered = X[random_features]\n",
    "\n",
    "            # get prediction of decision tree\n",
    "            predicted_class = decision_tree.make_prediction(X_filtered, decision_tree.root)\n",
    "\n",
    "            # append predicted class to predicted classes\n",
    "            predicted_classes.append(predicted_class)\n",
    "\n",
    "\n",
    "        # Majority voting for classification\n",
    "        return Counter(predicted_classes).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        return np.array([self.make_prediction(x) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "d737b313-b68e-4bb8-8cc3-c30b8aea88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "\n",
    "    def __init__(self, feature_index=None, threshold=None, left_node=None, right_node=None, info_gained=None, value=None):\n",
    "\n",
    "        # necessary attributes for a decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left_node = left_node\n",
    "        self.right_node = right_node\n",
    "        self.info_gained = info_gained\n",
    "\n",
    "        # necessary attributes for a leaf node\n",
    "        self.value = value\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=4):\n",
    "\n",
    "        # root will be used to traverse decision tree\n",
    "        self.root = None\n",
    "\n",
    "        # stopping conditions for build_tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def build_tree(self, dataset, current_depth=0):\n",
    "\n",
    "\n",
    "        X = dataset[:,:-1] # get all rows and all columns except the last one, which will be the column to classify\n",
    "        y = dataset[:,-1] # get all entries of last column \n",
    "\n",
    "        num_samples, num_features = np.shape(X)\n",
    "\n",
    "        # if either of these conditions is not met, the node returned will be a leaf node\n",
    "        if num_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "\n",
    "            if \"info_gained\" in best_split and best_split[\"info_gained\"] > 0:\n",
    "\n",
    "            \n",
    "                # recursively build left tree and right tree until sample is too small or depth is too high\n",
    "                left_tree = self.build_tree(best_split[\"less_than\"], current_depth+1)\n",
    "                right_tree = self.build_tree(best_split[\"greater_than\"], current_depth+1)\n",
    "    \n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], left_tree, right_tree, best_split[\"info_gained\"])\n",
    "\n",
    "        # else return a leaf node when data is pure\n",
    "\n",
    "        leaf_node_value = self.calculate_leaf_node_value(y)\n",
    "        return Node(value=leaf_node_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "\n",
    "        best_split = {}\n",
    "        max_info_gained = -float(\"inf\") #instantiating at -inf because every value is greater than -inf\n",
    "\n",
    "        # iteratively check every value of every feature as a candidate to be a split for decision node\n",
    "        for feature_idx in range(num_features):\n",
    "\n",
    "            possible_thresholds = np.unique(dataset[:, feature_idx]) # get unique list of feature values that can be used to split data into left and right node\n",
    "            for threshold in possible_thresholds:\n",
    "\n",
    "                # filter dataset based on category\n",
    "                less_than = dataset[dataset[:, feature_idx] <= threshold] \n",
    "                greater_than = dataset[dataset[:, feature_idx] > threshold]\n",
    "\n",
    "                # the threshold must split the data so that there is data above and below threshold, otherwise the split did nothing\n",
    "                if len(less_than) > 0 and len(greater_than) > 0:\n",
    "\n",
    "                    # extracting the class label column for each dataset\n",
    "                    y = dataset[:,-1]\n",
    "                    y_less_than = less_than[:,-1] \n",
    "                    y_greater_than = greater_than[:,-1]\n",
    "\n",
    "                    # calculating the amount of information gained, we are trying to maximize it\n",
    "                    info_gained = self.info_gained(y, y_less_than, y_greater_than)\n",
    "\n",
    "                    if info_gained > max_info_gained:\n",
    "\n",
    "                        # set new max info gained\n",
    "                        best_split[\"feature_index\"] = feature_idx\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"less_than\"] = less_than\n",
    "                        best_split[\"greater_than\"] = greater_than\n",
    "                        best_split[\"info_gained\"] = info_gained\n",
    "                        max_info_gained = info_gained\n",
    "                        \n",
    "        return best_split\n",
    "\n",
    "\n",
    "    def info_gained(self, y, y_less_than, y_greater_than, mode=\"entropy\"):\n",
    "\n",
    "        class_labels = np.unique(y) # in our case this will be 1 and 0\n",
    "\n",
    "        weight_less_than = len(y_less_than) / len(y)\n",
    "        weight_greater_than = len(y_greater_than) / len(y)\n",
    "\n",
    "        #calculates info gained\n",
    "        info_gained = self.entropy(y) - (weight_less_than * self.entropy(y_less_than) + weight_greater_than * self.entropy(y_greater_than))\n",
    "\n",
    "        return info_gained\n",
    "\n",
    "    def entropy(self, y):\n",
    "\n",
    "        class_labels = np.unique(y) # gets a list of possible classes \n",
    "        entropy = 0\n",
    "\n",
    "        for class_label in class_labels:\n",
    "\n",
    "            prob_of_class = np.count_nonzero(y == class_label) / len(y) # calculates probability that an element of y is the certain class\n",
    "            entropy += -1*prob_of_class*np.log2(prob_of_class) # calculates entropy\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    def calculate_leaf_node_value(self, y):\n",
    "\n",
    "        '''Find most common Y value to be the leaf node value'''\n",
    "\n",
    "        y = list(y)\n",
    "        return max(y, key=y.count)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        data = np.concatenate((X,np.array([[entry] for entry in y])), axis=1)\n",
    "        self.root = self.build_tree(data)\n",
    "\n",
    "    def make_prediction(self, X, node):\n",
    "\n",
    "        '''Get prediction for 1d array. Traverses through tree to find class that entry fits'''\n",
    "        \n",
    "        while node.value is None: # while we are on a decision node\n",
    "\n",
    "            feature_value = X[node.feature_index] # only focus on feature of X that node analyzes\n",
    "\n",
    "            if feature_value <= node.threshold:\n",
    "\n",
    "                node = node.left_node\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                node = node.right_node\n",
    "\n",
    "        return node.value\n",
    "        \n",
    "    def predict(self, X):\n",
    "\n",
    "        '''get multiple predictions for 2d array'''\n",
    "\n",
    "\n",
    "        return np.array([self.make_prediction(row, self.root) for row in X])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d0adbe2a-6bd3-4deb-a4a2-2839f765b110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "rf_model = DecisionTreeClassifier()\n",
    "rf_model.fit(iris.data, iris.target)\n",
    "rf_model.predict(np.array([[5.9,3,5.1,1.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "19622092-1642-4699-81ac-dd5999b438a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(iris.data, iris.target)\n",
    "rf_model.predict(np.array([[5.9,3,5.1,1.8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2b094709-5dbc-41b8-acb7-fa8380839c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(num_trees=15, max_features=4)  # Example parameters\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bede6e-cf2e-4caf-8222-62a211b52582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4ca6f-9520-486f-8255-1b274e302848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
